{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Digital Gait Laboratory: Hybrid Architecture (Raw + Advanced Features)\n",
    "\n",
    "**Course:** Wearable Computing & Gait Analysis  \n",
    "**Dataset:** UCI HAPT (Activity Recognition + Postural Transitions)\n",
    "\n",
    "### üéØ Objective\n",
    "We are upgrading the pipeline to improve accuracy. Instead of relying solely on Deep Learning to find patterns in raw data, we will explicitely calculate **Statistical Features** (Mean, Variance, Energy) and feed them into the network alongside the raw signal.\n",
    "\n",
    "**The Hybrid Pipeline:**\n",
    "1.  **Input:** Raw Sensor Window (128 time steps).\n",
    "2.  **Branch A (CNN):** Reads the raw signal to find temporal patterns (e.g., heel strike shape).\n",
    "3.  **Branch B (MLP):** Reads calculated statistics. **Updated:** Now includes Frequency Domain (FFT), Correlation, **Jerk**, **Entropy**, and **IQR** features.\n",
    "4.  **Fusion:** Combines both to predict the activity.\n",
    "5.  **Phyphox Inference:** Applies the exact same feature math to your smartphone data.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.fft import fft\n",
    "from scipy.stats import iqr, entropy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration\n",
    "FS = 50              # Sampling Frequency (Hz)\n",
    "WINDOW_SIZE = 128    # 2.56 seconds per window\n",
    "OVERLAP = 64         # 50% overlap\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "LR = 0.0005\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì• 2. Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\"\n",
    "zip_path = \"HAPT_Data_Set.zip\"\n",
    "extract_folder = \"HAPT_Dataset\"\n",
    "\n",
    "if not os.path.exists(extract_folder):\n",
    "    print(\"Downloading dataset...\")\n",
    "    !wget -q $dataset_url -O $zip_path\n",
    "    print(\"Extracting...\")\n",
    "    !unzip -q $zip_path -d $extract_folder\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê 3. Feature Engineering (The \"Math\" Department)\n",
    "This is the most critical upgrade. We define a function `extract_features` that runs on **every window** of data (both training and Phyphox).\n",
    "\n",
    "**New Features Added (based on features_info.txt):**\n",
    "1.  **Jerk Signals:** The derivative of acceleration ($dA/dt$). Critical for measuring \"smoothness\" of movement.\n",
    "2.  **Spectral Entropy:** Measures the \"randomness\" of the frequency distribution. (Walking = Low Entropy, Random Shaking = High Entropy).\n",
    "3.  **IQR:** Interquartile Range, a robust measure of spread.\n",
    "4.  **Magnitudes:** Euclidean norm of Jerk signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_gravity(data, fs=50):\n",
    "    # Low-pass filter to isolate gravity\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = 0.3 / nyq\n",
    "    b, a = butter(3, normal_cutoff, btype='low', analog=False)\n",
    "    gravity = lfilter(b, a, data, axis=0)\n",
    "    body_acc = data - gravity\n",
    "    return body_acc, gravity\n",
    "\n",
    "def extract_features(window):\n",
    "    \"\"\"\n",
    "    Input: window shape (128, 9) \n",
    "           [BodyAccXYZ (0-3), GyroXYZ (3-6), TotalAccXYZ (6-9)]\n",
    "    Output: Feature vector (numpy array)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # --- PREPARE SIGNALS ---\n",
    "    body_acc = window[:, 0:3]\n",
    "    gyro = window[:, 3:6]\n",
    "    total_acc = window[:, 6:9]\n",
    "    \n",
    "    # 1. Jerk Signals (Derivative over time)\n",
    "    # We pad with 0 at the start to keep length 128\n",
    "    acc_jerk = np.diff(body_acc, axis=0, prepend=body_acc[0:1,:])\n",
    "    gyro_jerk = np.diff(gyro, axis=0, prepend=gyro[0:1,:])\n",
    "    \n",
    "    # 2. Magnitudes (Euclidean Norm)\n",
    "    acc_mag = np.linalg.norm(body_acc, axis=1)\n",
    "    gyro_mag = np.linalg.norm(gyro, axis=1)\n",
    "    acc_jerk_mag = np.linalg.norm(acc_jerk, axis=1)\n",
    "    gyro_jerk_mag = np.linalg.norm(gyro_jerk, axis=1)\n",
    "    \n",
    "    # Collect all temporal signals to loop over\n",
    "    # 3 Axes Acc + 3 Axes Gyro + 3 Axes AccJerk + 3 Axes GyroJerk + 4 Mags = 16 signals\n",
    "    signals = [body_acc, gyro, acc_jerk, gyro_jerk]\n",
    "    mags = [acc_mag, gyro_mag, acc_jerk_mag, gyro_jerk_mag]\n",
    "    \n",
    "    # --- TIME DOMAIN FEATURES ---\n",
    "    for signal_group in signals:\n",
    "        # signal_group shape: (128, 3)\n",
    "        features.append(np.mean(signal_group, axis=0))      # Mean\n",
    "        features.append(np.std(signal_group, axis=0))       # Std\n",
    "        features.append(np.max(signal_group, axis=0))       # Max\n",
    "        features.append(np.min(signal_group, axis=0))       # Min\n",
    "        features.append(iqr(signal_group, axis=0))          # IQR (Robust Spread)\n",
    "        features.append(np.sum(signal_group**2, axis=0) / len(window)) # Energy\n",
    "        \n",
    "    for mag_signal in mags:\n",
    "        # mag_signal shape: (128,)\n",
    "        features.append([np.mean(mag_signal)])\n",
    "        features.append([np.std(mag_signal)])\n",
    "        features.append([np.max(mag_signal)])\n",
    "        features.append([iqr(mag_signal)])\n",
    "        \n",
    "    # --- FREQUENCY DOMAIN FEATURES ---\n",
    "    # FFT on BodyAcc and Gyro (common practice)\n",
    "    for signal_group in [body_acc, gyro]:\n",
    "        for i in range(3): # X, Y, Z\n",
    "            col = signal_group[:, i]\n",
    "            fft_vals = np.abs(fft(col))[:len(window)//2]\n",
    "            \n",
    "            # Spectral Energy\n",
    "            features.append([np.mean(fft_vals)])\n",
    "            \n",
    "            # Spectral Entropy (Treat power spectrum as probability distribution)\n",
    "            psd = fft_vals / (np.sum(fft_vals) + 1e-9) # Normalize\n",
    "            features.append([entropy(psd)])\n",
    "            \n",
    "            # Dominant Frequency\n",
    "            features.append([np.argmax(fft_vals)])\n",
    "\n",
    "    # --- CORRELATION (Axis Synchronization) ---\n",
    "    # Acc X-Y, X-Z, Y-Z\n",
    "    features.append([np.corrcoef(body_acc[:,0], body_acc[:,1])[0,1]])\n",
    "    features.append([np.corrcoef(body_acc[:,0], body_acc[:,2])[0,1]])\n",
    "    features.append([np.corrcoef(body_acc[:,1], body_acc[:,2])[0,1]])\n",
    "    \n",
    "    # --- ANGLE (Posture detection) ---\n",
    "    # Angle between Mean Body Acc vector and Mean Gravity vector\n",
    "    # Gravity is approx equal to the mean of the Total Acc signal (since body acc avg is 0)\n",
    "    mean_total_acc = np.mean(total_acc, axis=0) \n",
    "    mean_body_acc = np.mean(body_acc, axis=0)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    num = np.dot(mean_total_acc, mean_body_acc)\n",
    "    denom = np.linalg.norm(mean_total_acc) * np.linalg.norm(mean_body_acc) + 1e-9\n",
    "    angle = np.arccos(np.clip(num/denom, -1.0, 1.0))\n",
    "    features.append([angle])\n",
    "\n",
    "    # Flatten\n",
    "    return np.nan_to_num(np.concatenate([np.atleast_1d(f) for f in features]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è 4. Data Pipeline\n",
    "We load the raw data, segment it, AND run our feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Hybrid Dataset (Raw + Features)...\n",
      "Raw Data Shape: (10890, 9, 128)\n",
      "Feature Data Shape: (10890, 110)\n",
      "Labels Shape: (10890,)\n"
     ]
    }
   ],
   "source": [
    "def load_hapt_hybrid(base_path):\n",
    "    raw_path = os.path.join(base_path, \"RawData\")\n",
    "    labels_df = pd.read_csv(os.path.join(raw_path, \"labels.txt\"), sep='\\\\s+', header=None)\n",
    "    labels_df.columns = [\"exp\", \"user\", \"act\", \"start\", \"end\"]\n",
    "    \n",
    "    raw_windows = []\n",
    "    handcrafted_features = []\n",
    "    labels = []\n",
    "\n",
    "    for exp_id in labels_df['exp'].unique():\n",
    "        user_id = labels_df[labels_df['exp'] == exp_id]['user'].iloc[0]\n",
    "        acc_file = f\"acc_exp{exp_id:02d}_user{user_id:02d}.txt\"\n",
    "        gyro_file = f\"gyro_exp{exp_id:02d}_user{user_id:02d}.txt\"\n",
    "        \n",
    "        # Load\n",
    "        acc_raw = pd.read_csv(os.path.join(raw_path, acc_file), sep='\\\\s+', header=None).values\n",
    "        gyro_raw = pd.read_csv(os.path.join(raw_path, gyro_file), sep='\\\\s+', header=None).values\n",
    "        \n",
    "        # Preprocess\n",
    "        body_acc, gravity = separate_gravity(acc_raw, FS)\n",
    "        full_data = np.hstack([body_acc, gyro_raw, acc_raw])\n",
    "        \n",
    "        # Segment\n",
    "        exp_labels = labels_df[labels_df['exp'] == exp_id]\n",
    "        for _, row in exp_labels.iterrows():\n",
    "            start, end, act_id = row['start'], row['end'], row['act']\n",
    "            segment = full_data[start:end]\n",
    "            \n",
    "            for i in range(0, len(segment) - WINDOW_SIZE, OVERLAP):\n",
    "                window = segment[i : i + WINDOW_SIZE]\n",
    "                \n",
    "                # 1. Store Raw Window\n",
    "                raw_windows.append(window)\n",
    "                \n",
    "                # 2. Calculate Features\n",
    "                feats = extract_features(window)\n",
    "                handcrafted_features.append(feats)\n",
    "                \n",
    "                # 3. Store Label\n",
    "                labels.append(act_id - 1)\n",
    "\n",
    "    return np.array(raw_windows), np.array(handcrafted_features), np.array(labels)\n",
    "\n",
    "print(\"Processing Hybrid Dataset (Raw + Features)...\")\n",
    "X_raw, X_feat, y_all = load_hapt_hybrid(\"HAPT_Dataset\")\n",
    "\n",
    "# Normalize Features (Critical for MLP)\n",
    "scaler = StandardScaler()\n",
    "X_feat = scaler.fit_transform(X_feat)\n",
    "\n",
    "# Transpose Raw for PyTorch (N, Channels, Time)\n",
    "X_raw = np.transpose(X_raw, (0, 2, 1))\n",
    "\n",
    "print(f\"Raw Data Shape: {X_raw.shape}\")\n",
    "print(f\"Feature Data Shape: {X_feat.shape}\")\n",
    "print(f\"Labels Shape: {y_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† 5. Hybrid Neural Network\n",
    "This network has two \"heads\":\n",
    "1.  **ConvNet Head:** Processes the raw 128-step time series.\n",
    "2.  **Dense Head:** Processes the feature vector.\n",
    "3.  **Fusion Layer:** Concatenates both and makes the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridGaitNet(nn.Module):\n",
    "    def __init__(self, n_raw_channels=9, n_features=63, n_classes=12):\n",
    "        super(HybridGaitNet, self).__init__()\n",
    "        \n",
    "        # --- Branch A: Raw Signal (CNN) ---\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(n_raw_channels, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            \n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1), # Squeeze time -> (Batch, 128, 1)\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # --- Branch B: Handcrafted Features (MLP) ---\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # --- Fusion ---\n",
    "        # CNN out (128) + MLP out (64) = 192\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(128 + 64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_raw, x_feat):\n",
    "        # Pass through separate branches\n",
    "        out_cnn = self.cnn(x_raw)\n",
    "        out_mlp = self.mlp(x_feat)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat((out_cnn, out_mlp), dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        return self.fusion(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è 6. Training the Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 110 features per window.\n",
      "Starting Hybrid Training...\n",
      "Epoch [1/40] Loss: 0.6133 | Test Acc: 90.77%\n",
      "Epoch [2/40] Loss: 0.2118 | Test Acc: 93.34%\n",
      "Epoch [3/40] Loss: 0.1660 | Test Acc: 93.89%\n",
      "Epoch [4/40] Loss: 0.1516 | Test Acc: 93.57%\n",
      "Epoch [5/40] Loss: 0.1416 | Test Acc: 94.12%\n",
      "Epoch [6/40] Loss: 0.1298 | Test Acc: 93.76%\n",
      "Epoch [7/40] Loss: 0.1264 | Test Acc: 95.18%\n",
      "Epoch [8/40] Loss: 0.1203 | Test Acc: 94.90%\n",
      "Epoch [9/40] Loss: 0.1137 | Test Acc: 94.95%\n",
      "Epoch [10/40] Loss: 0.1132 | Test Acc: 95.32%\n",
      "Epoch [11/40] Loss: 0.1118 | Test Acc: 95.32%\n",
      "Epoch [12/40] Loss: 0.1062 | Test Acc: 95.00%\n",
      "Epoch [13/40] Loss: 0.1011 | Test Acc: 95.55%\n",
      "Epoch [14/40] Loss: 0.1093 | Test Acc: 94.49%\n",
      "Epoch [15/40] Loss: 0.1097 | Test Acc: 95.41%\n",
      "Epoch [16/40] Loss: 0.1010 | Test Acc: 95.73%\n",
      "Epoch [17/40] Loss: 0.1015 | Test Acc: 95.41%\n",
      "Epoch [18/40] Loss: 0.0969 | Test Acc: 95.91%\n",
      "Epoch [19/40] Loss: 0.0931 | Test Acc: 95.91%\n",
      "Epoch [20/40] Loss: 0.0986 | Test Acc: 96.33%\n",
      "Epoch [21/40] Loss: 0.0909 | Test Acc: 95.91%\n",
      "Epoch [22/40] Loss: 0.0907 | Test Acc: 96.01%\n",
      "Epoch [23/40] Loss: 0.0887 | Test Acc: 96.01%\n",
      "Epoch [24/40] Loss: 0.0870 | Test Acc: 96.05%\n",
      "Epoch [25/40] Loss: 0.0855 | Test Acc: 95.50%\n",
      "Epoch [26/40] Loss: 0.0921 | Test Acc: 96.33%\n",
      "Epoch [27/40] Loss: 0.0895 | Test Acc: 95.87%\n",
      "Epoch [28/40] Loss: 0.0872 | Test Acc: 96.10%\n",
      "Epoch [29/40] Loss: 0.0818 | Test Acc: 96.01%\n",
      "Epoch [30/40] Loss: 0.0839 | Test Acc: 96.01%\n",
      "Epoch [31/40] Loss: 0.0779 | Test Acc: 96.60%\n",
      "Epoch [32/40] Loss: 0.0803 | Test Acc: 96.60%\n",
      "Epoch [33/40] Loss: 0.0796 | Test Acc: 96.33%\n",
      "Epoch [34/40] Loss: 0.0802 | Test Acc: 96.19%\n",
      "Epoch [35/40] Loss: 0.0738 | Test Acc: 96.28%\n",
      "Epoch [36/40] Loss: 0.0804 | Test Acc: 95.59%\n",
      "Epoch [37/40] Loss: 0.0782 | Test Acc: 96.24%\n",
      "Epoch [38/40] Loss: 0.0788 | Test Acc: 96.24%\n",
      "Epoch [39/40] Loss: 0.0748 | Test Acc: 96.33%\n",
      "Epoch [40/40] Loss: 0.0753 | Test Acc: 96.24%\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset that returns TWO inputs\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, x_raw, x_feat, y):\n",
    "        self.x_raw = torch.FloatTensor(x_raw)\n",
    "        self.x_feat = torch.FloatTensor(x_feat)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.x_raw[idx], self.x_feat[idx], self.y[idx]\n",
    "\n",
    "# Split\n",
    "Xr_train, Xr_test, Xf_train, Xf_test, y_train, y_test = train_test_split(\n",
    "    X_raw, X_feat, y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(HybridDataset(Xr_train, Xf_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(HybridDataset(Xr_test, Xf_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Setup\n",
    "n_feats = X_feat.shape[1]\n",
    "print(f\"Detected {n_feats} features per window.\")\n",
    "model = HybridGaitNet(n_features=n_feats).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Train\n",
    "train_losses, test_accs = [], []\n",
    "print(\"Starting Hybrid Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for r_in, f_in, labels in train_loader:\n",
    "        r_in, f_in, labels = r_in.to(device), f_in.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(r_in, f_in)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Test\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for r_in, f_in, labels in test_loader:\n",
    "            r_in, f_in, labels = r_in.to(device), f_in.to(device), labels.to(device)\n",
    "            outputs = model(r_in, f_in)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            \n",
    "    acc = 100 * correct / total\n",
    "    test_accs.append(acc)\n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {train_losses[-1]:.4f} | Test Acc: {acc:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"hybrid_gait_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì≤ 7. Phyphox: Hybrid Inference\n",
    "Here we replicate the entire pipeline for your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Accelerometer.csv...\n",
      "\n",
      "--- üìä Hybrid Classification Results ---\n",
      "WALKING: 66.7%\n",
      "WALK_UP: 5.6%\n",
      "WALK_DOWN: 2.8%\n",
      "SIT_TO_LIE: 5.6%\n",
      "STAND_TO_LIE: 19.4%\n"
     ]
    }
   ],
   "source": [
    "HAPT_LABELS = {0: \"WALKING\", 1: \"WALK_UP\", 2: \"WALK_DOWN\", 3: \"SITTING\", 4: \"STANDING\", 5: \"LAYING\", \n",
    "               6: \"STAND_TO_SIT\", 7: \"SIT_TO_STAND\", 8: \"SIT_TO_LIE\", 9: \"LIE_TO_SIT\", 10: \"STAND_TO_LIE\", 11: \"LIE_TO_STAND\"}\n",
    "\n",
    "def classify_phyphox_hybrid(acc_path, gyro_path):\n",
    "    print(f\"Processing {acc_path}...\")\n",
    "    try:\n",
    "        df_acc = pd.read_csv(acc_path)\n",
    "        df_gyro = pd.read_csv(gyro_path)\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Files not found.\")\n",
    "        return\n",
    "    \n",
    "    # 1. Clean & Resample\n",
    "    def clean_cols(df):\n",
    "        col_map = {}\n",
    "        for c in df.columns:\n",
    "            c_lower = c.lower()\n",
    "            if 'time' in c_lower: col_map[c] = 'time'\n",
    "            elif 'x' in c_lower: col_map[c] = 'x'\n",
    "            elif 'y' in c_lower: col_map[c] = 'y'\n",
    "            elif 'z' in c_lower: col_map[c] = 'z'\n",
    "        return df.rename(columns=col_map)[['time', 'x', 'y', 'z']]\n",
    "\n",
    "    df_acc = clean_cols(df_acc)\n",
    "    df_gyro = clean_cols(df_gyro)\n",
    "\n",
    "    df_acc[['x', 'y', 'z']] = df_acc[['x', 'y', 'z']] / 9.80665\n",
    "    \n",
    "    # Interpolate to 50Hz\n",
    "    t_start = max(df_acc['time'].min(), df_gyro['time'].min())\n",
    "    t_end = min(df_acc['time'].max(), df_gyro['time'].max())\n",
    "    new_time = np.arange(t_start, t_end, 1/50.0)\n",
    "\n",
    "    def resample(df, t_target):\n",
    "        df = df.set_index('time')\n",
    "        return df.reindex(df.index.union(t_target)).interpolate(method='linear').reindex(t_target).values\n",
    "\n",
    "    acc_data = resample(df_acc, new_time)\n",
    "    gyro_data = resample(df_gyro, new_time)\n",
    "\n",
    "    # 2. Gravity Sep\n",
    "    body_acc, gravity = separate_gravity(acc_data, fs=50)\n",
    "    full_signal = np.hstack([body_acc, gyro_data, acc_data])\n",
    "    \n",
    "    # 3. Windowing & Feature Extraction\n",
    "    raw_wins = []\n",
    "    feat_wins = []\n",
    "    \n",
    "    for i in range(0, len(full_signal) - WINDOW_SIZE, OVERLAP):\n",
    "        win = full_signal[i : i + WINDOW_SIZE]\n",
    "        raw_wins.append(win)\n",
    "        feat_wins.append(extract_features(win))\n",
    "        \n",
    "    if not raw_wins: return\n",
    "    \n",
    "    # 4. Prepare Tensors\n",
    "    # Raw: (N, 9, 128)\n",
    "    X_raw_phy = np.array(raw_wins).transpose(0, 2, 1)\n",
    "    # Feat: Normalize using the SAME scaler from training\n",
    "    X_feat_phy = scaler.transform(np.array(feat_wins))\n",
    "    \n",
    "    # 5. Inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        r_in = torch.FloatTensor(X_raw_phy).to(device)\n",
    "        f_in = torch.FloatTensor(X_feat_phy).to(device)\n",
    "        outputs = model(r_in, f_in)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "    # 6. Report\n",
    "    print(\"\\n--- üìä Hybrid Classification Results ---\")\n",
    "    unique, counts = np.unique(preds.cpu().numpy(), return_counts=True)\n",
    "    total = sum(counts)\n",
    "    for cls_idx, count in zip(unique, counts):\n",
    "        print(f\"{HAPT_LABELS[cls_idx]}: {(count/total)*100:.1f}%\")\n",
    "\n",
    "classify_phyphox_hybrid(\"Accelerometer.csv\", \"Gyroscope.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
